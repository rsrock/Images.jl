{
    "docs": [
        {
            "location": "/", 
            "text": "Images.jl\n\n\nAn image processing library for \nJulia\n.\n\n\n\n\nInstallation\n\n\nInstall via the package manager,\n\n\nPkg.add(\nImages\n)\n\n\n\n\nIt's helpful to have ImageMagick installed on your system, as Images relies on it for reading and writing many common image types.  ImageMagick \nshould\n be installed for you automatically. In case of trouble, more details about manual installation and troubleshooting can be found in the \ninstallation help\n. Mac users in particular seem to have trouble; you may find \ndebugging Homebrew\n useful.\n\n\n\n\nPackage interactions\n\n\nA few other packages define overlapping functions or types (\nPyPlot\n defines \nimread\n, and \nWinston\n defines \nImage\n).  When using both Images and these packages, you can always specify which version you want with \nImages.imread(\"myimage.png\")\n.\n\n\n\n\nImage viewing\n\n\nIf you're using the \nIJulia\n notebook, images will be displayed \nautomatically\n.\n\n\nJulia code for the display of images can be found in \nImageView\n.  Installation of this package is recommended but not required.\n\n\n\n\nTestImages\n\n\nWhen testing ideas or just following along with the documentation, it can be useful to have some images to work with.  The \nTestImages\n package bundles several \"standard\" images for you.  To load one of the images from this package, say\n\n\nusing TestImages\nimg = testimage(\nmandrill\n)\n\n\n\n\nThe examples below will assume you're loading a particular file from your disk, but you can substitute those commands with \ntestimage\n.\n\n\n\n\nGetting started\n\n\nFor these examples you'll need to install both \nImages\n and \nImageView\n. Depending on your task, it's also very useful to have two other packages loaded, \nColors\n and \nFixedPointNumbers\n.  Load the code for all of these packages with\n\n\nusing Images, Colors, FixedPointNumbers, ImageView\n\n\n\n\n\n\nLoading your first image: how images are represented\n\n\nYou likely have a number of images already at your disposal, and you can use these, TestImages.jl, or run \nreadremote.jl\n in the \ntest/\n directory.  (This requires an internet connection.)  These will be deposited inside an \nImages\n directory inside your temporary directory (e.g., \n/tmp\n on Linux systems). The \n\"rose.png\"\n image in this example comes from the latter.\n\n\nLet's begin by reading an image from a file:\n\n\njulia\n img = imread(\nrose.png\n)\nRGB Image with:\n  data: 70x46 Array{RGB{UFixed{Uint8,8}},2}\n  properties:\n    IMcs: sRGB\n    spatialorder:  x y\n    pixelspacing:  1 1\n\n\n\n\nIf you're using Images through IJulia, rather than this text output you probably see the image itself.  This is nice, but often it's quite helpful to see the structure of these Image objects.  This happens automatically at the REPL; within IJulia you can call\n\n\nshow(img)\n\n\n\n\nto see the output above.\n\n\nThe first line tells you that this is an RGB image. It is stored as a two-dimensional \nArray\n of \nRGB{UFixed{Uint8,8}}\n. To see what this pixel type is, we can do the following:\n\n\njulia\n img[1,1]\nRGB{UFixed8}(0.188,0.184,0.176)\n\n\n\n\nThis extracts the first pixel, the one visually at the upper-left of the image. You can see that an \nRGB\n (which comes from the \nColors\n package) is a triple of values. The \nUFixed8\n number type (which comes from the \nFixedPointNumbers\n package), and whose long name is \nUFixed{Uint8,8}\n) represents fractional numbers, those that can encode values that lie between 0 and 1, using just 1 byte (8 bits).  If you've previously used other image processing libraries, you may be used to thinking of two basic image types, floating point-valued and integer-valued. In those libraries, \"saturated\" (the color white for an RGB image) would be represented by \n1.0\n for floating point-valued images, 255 for a \nUint8\n image, and \n0x0fff\n for an image collected by a 12-bit camera. \nImages.jl\n, via Colors and FixedPointNumbers, unifies these so that \n1\n always means saturated, no matter whether the element type is \nFloat64\n, \nUFixed8\n, or \nUFixed12\n.  This makes it easier to write generic algorithms and visualization code, while still allowing one to use efficient (and C-compatible) raw representations.\n\n\nYou can see that this image has \nproperties\n, of which there are three: \n\"IMcs\"\n, \n\"spatialorder\"\n and \n\"pixelspacing\"\n.  We'll talk more about the latter two in the next section.  The \n\"IMcs\"\n is really for internal use by ImageMagick; it says that the colorspace is \n\"sRGB\"\n, although (depending on which version of the library you have) you may see it say \n\"RGB\"\n.  Such differences are due to \nchanges\n in how ImageMagick handles colorspaces, and the fact that both older and newer versions of the library are still widespread.\n\n\nYou can retrieve the properties using \nprops = properties(img)\n. This returns the dictionary used by \nimg\n; any modifications you make to \nprops\n will update the properties of \nimg\n.\n\n\nLikewise, given an Image \nimg\n, you can access the underlying array with\n\n\nA = data(img)\n\n\n\n\nThis is handy for those times when you want to call an algorithm that is implemented only for \nArray\ns. At the end, however, you may want to restore the contextual information available in an Image. While you can use the \nImage\n constructor directly, two alternatives can be convenient:\n\n\nimgc = copyproperties(img, A)\nimgs = shareproperties(img, A)\n\n\n\n\nimgc\n has its own properties dictionary, initialized to be a copy of the one used by \nimg\n.  In contrast, \nimgs\n shares a properties dictionary with \nimg\n; any modification to the properties of \nimg\n will also modify them for \nimgs\n. Use either as appropriate to your circumstance.\n\n\nThe Images package is designed to work with either plain arrays or with Image types\u2013-in general, though, you're probably best off leaving things as an Image, particularly if you work with movies, 3d images, or other more complex objects.\n\n\n\n\nStorage order and changing the representation of images\n\n\nIn the example above, the \n\"spatialorder\"\n property has value \n[\"x\", \"y\"]\n. This indicates that the image data are in \"horizontal-major\" order, meaning that a pixel at spatial location \n(x,y)\n would be addressed as \nimg[x,y]\n rather than \nimg[y,x]\n. \n[\"y\", \"x\"]\n would indicate vertical-major.  Consequently, this image is 70 pixels wide and 46 pixels high.\n\n\nImages returns this image in horizontal-major order because this is how it was stored on disk.  Because the Images package is designed to scale to terabyte-sized images, a general philosophy is to work with whatever format users provide without forcing changes to the raw array representation. Consequently, when you load an image, its representation will match that used in the file.\n\n\nOf course, if you prefer to work with plain arrays, you can convert it:\n\n\njulia\n imA = convert(Array, img);\n\njulia\n summary(imA)\n\n46x70 Array{RGB{UFixed{Uint8,8}},2}\n\n\n\n\n\nYou can see that this permuted the dimensions into vertical-major order, consistent with the column-major order with which Julia stores \nArrays\n. Note that this preserved the element type, returning an \nArray{RGB}\n.  If you prefer to extract into an array of plain numbers in color-last order (typical of Matlab), you can use\n\n\njulia\n imsep = separate(img)\nRGB Image with:\n  data: 46x70x3 Array{UFixed{Uint8,8},3}\n  properties:\n    IMcs: sRGB\n    colorspace: RGB\n    colordim: 3\n    spatialorder:  y x\n    pixelspacing:  1 1\n\n\n\n\nYou can see that \n\"spatialorder\"\n was changed to reflect the new layout, and that two new properties were added: \n\"colordim\"\n, which specifies which dimension of the array is used to encode color, and \n\"colorspace\"\n so you know how to interpret these colors.\n\n\nCompare this to\n\n\njulia\n imr = reinterpret(UFixed8, img)\nRGB Image with:\n  data: 3x70x46 Array{UFixed{Uint8,8},3}\n  properties:\n    IMcs: sRGB\n    colorspace: RGB\n    colordim: 1\n    spatialorder:  x y\n    pixelspacing:  1 1\n\n\n\n\nreinterpret\n gives you a new view of the same underlying memory as \nimg\n, whereas \nconvert(Array, img)\n and \nseparate(img)\n create new arrays if the memory-layout needs alteration.\n\n\nYou can go back to using Colors to encode your image this way:\n\n\njulia\n imcomb = convert(Image{RGB}, imsep)\nRGB Image with:\n  data: 46x70 Array{RGB{UFixed{Uint8,8}},2}\n  properties:\n    IMcs: sRGB\n    spatialorder:  y x\n    pixelspacing:  1 1\n\n\n\n\nor even change to a new colorspace like this:\n\n\njulia\n imhsv = convert(Image{HSV}, float32(img))\nHSV Image with:\n  data: 70x46 Array{HSV{Float32},2}\n  properties:\n    IMcs: sRGB\n    spatialorder:  x y\n    pixelspacing:  1 1\n\n\n\n\nMany of the colorspaces supported by Colors (or rather its base package, ColorTypes) need a wider range of values than \n[0,1]\n, so it's necessary to convert to floating point.\n\n\nIf you say \nview(imhsv)\n, you may be surprised to see something that looks like the original RGB image. Since the colorspace is known, it converts to RGB before rendering it. If, for example, you wanted to see what a \"pure-V\" image looks like, you can do this:\n\n\nimv = shareproperties(imhsv, [HSV(0, 0, imhsv[i,j].v) for i = 1:size(imhsv,1),j = 1:size(imhsv,2)])\nview(imv)\n\n\n\n\nand a pure-H image like this:\n\n\nimh = shareproperties(imhsv, [HSV(imhsv[i,j].h, 0.5, 0.5) for i = 1:size(imhsv,1),j = 1:size(imhsv,2)])\nview(imh)\n\n\n\n\n(Hue without saturation or value generates gray or black, so we used a constant different from zero for these parameters.)\n\n\n \n\n\n\nOf course, you can combine these commands, for example\n\n\nA = reinterpret(Uint8, data(img))\n\n\n\n\nwill, for a \nRGB{UFixed8}\n image, return a raw 3d array.  This can be useful if you want to interact with external code (a C-library, for example).  Assuming you don't want to lose orientation information, you can wrap a returned array \nB\n as \nshareproperties(img, B)\n.\n\n\n\n\nOther properties, and usage of Units\n\n\nThe \n\"pixelspacing\"\n property informs ImageView that this image has an aspect ratio 1.  In scientific or medical imaging, you can use actual units to encode this property, for example through the \nSIUnits\n package.  For example, if you're doing microscopy you might specify\n\n\nusing SIUnits\nimg[\npixelspacing\n] = [0.32Micro*Meter,0.32Micro*Meter]\n\n\n\n\nIf you're performing three-dimensional imaging, you might set different values for the different axes:\n\n\nusing SIUnits.ShortUnits\nmriscan[\npixelspacing\n] = [0.2mm, 0.2mm, 2mm]\n\n\n\n\nImageView includes facilities for scale bars, and by supplying your pixel spacing you can ensure that the scale bars are accurate.\n\n\n\n\nA brief demonstration of image processing\n\n\nNow let's work through a more sophisticated example:\n\n\nusing Images, TestImages, ImageView\nimg = testimage(\nmandrill\n)\nview(img)\n# Let's do some blurring\nkern = ones(Float32,7,7)/49\nimgf = imfilter(img, kern)\nview(imgf)\n# Let's make an oversaturated image\nimgs = 2imgf\nview(imgs)\n\n\n\n\n \n\n\n\n\n\nFurther documentation\n\n\nDetailed documentation about the design of the library and the available functions can be found in the navigation list to the right. Here are some of the topics available:\n\n\n\n\nThe \ncore\n representation of images\n\n\nFunction reference\n\n\nOverlays\n, a type for combining multiple grayscale arrays   into a single color array\n\n\n\n\n\n\nCredits\n\n\nElements of this package descend from \"image.jl\" that once lived in Julia's \nextras/\n directory.  That file had several authors, of which the primary were Jeff Bezanson, Stefan Kroboth, Tim Holy, Mike Nolta, and Stefan Karpinski.  This repository has been quite heavily reworked; the current package maintainer is Tim Holy, and important contributions have been made by Ron Rock, Kevin Squire, Lucas Beyer, Elliot Saba, Isaiah Norton, Daniel Perry, Waldir Pimenta, Tobias Knopp, Jason Merrill, Dahua Lin, and several others.", 
            "title": "Home"
        }, 
        {
            "location": "/#imagesjl", 
            "text": "An image processing library for  Julia .", 
            "title": "Images.jl"
        }, 
        {
            "location": "/#installation", 
            "text": "Install via the package manager,  Pkg.add( Images )  It's helpful to have ImageMagick installed on your system, as Images relies on it for reading and writing many common image types.  ImageMagick  should  be installed for you automatically. In case of trouble, more details about manual installation and troubleshooting can be found in the  installation help . Mac users in particular seem to have trouble; you may find  debugging Homebrew  useful.", 
            "title": "Installation"
        }, 
        {
            "location": "/#package-interactions", 
            "text": "A few other packages define overlapping functions or types ( PyPlot  defines  imread , and  Winston  defines  Image ).  When using both Images and these packages, you can always specify which version you want with  Images.imread(\"myimage.png\") .", 
            "title": "Package interactions"
        }, 
        {
            "location": "/#image-viewing", 
            "text": "If you're using the  IJulia  notebook, images will be displayed  automatically .  Julia code for the display of images can be found in  ImageView .  Installation of this package is recommended but not required.", 
            "title": "Image viewing"
        }, 
        {
            "location": "/#testimages", 
            "text": "When testing ideas or just following along with the documentation, it can be useful to have some images to work with.  The  TestImages  package bundles several \"standard\" images for you.  To load one of the images from this package, say  using TestImages\nimg = testimage( mandrill )  The examples below will assume you're loading a particular file from your disk, but you can substitute those commands with  testimage .", 
            "title": "TestImages"
        }, 
        {
            "location": "/#getting-started", 
            "text": "For these examples you'll need to install both  Images  and  ImageView . Depending on your task, it's also very useful to have two other packages loaded,  Colors  and  FixedPointNumbers .  Load the code for all of these packages with  using Images, Colors, FixedPointNumbers, ImageView", 
            "title": "Getting started"
        }, 
        {
            "location": "/#loading-your-first-image-how-images-are-represented", 
            "text": "You likely have a number of images already at your disposal, and you can use these, TestImages.jl, or run  readremote.jl  in the  test/  directory.  (This requires an internet connection.)  These will be deposited inside an  Images  directory inside your temporary directory (e.g.,  /tmp  on Linux systems). The  \"rose.png\"  image in this example comes from the latter.  Let's begin by reading an image from a file:  julia  img = imread( rose.png )\nRGB Image with:\n  data: 70x46 Array{RGB{UFixed{Uint8,8}},2}\n  properties:\n    IMcs: sRGB\n    spatialorder:  x y\n    pixelspacing:  1 1  If you're using Images through IJulia, rather than this text output you probably see the image itself.  This is nice, but often it's quite helpful to see the structure of these Image objects.  This happens automatically at the REPL; within IJulia you can call  show(img)  to see the output above.  The first line tells you that this is an RGB image. It is stored as a two-dimensional  Array  of  RGB{UFixed{Uint8,8}} . To see what this pixel type is, we can do the following:  julia  img[1,1]\nRGB{UFixed8}(0.188,0.184,0.176)  This extracts the first pixel, the one visually at the upper-left of the image. You can see that an  RGB  (which comes from the  Colors  package) is a triple of values. The  UFixed8  number type (which comes from the  FixedPointNumbers  package), and whose long name is  UFixed{Uint8,8} ) represents fractional numbers, those that can encode values that lie between 0 and 1, using just 1 byte (8 bits).  If you've previously used other image processing libraries, you may be used to thinking of two basic image types, floating point-valued and integer-valued. In those libraries, \"saturated\" (the color white for an RGB image) would be represented by  1.0  for floating point-valued images, 255 for a  Uint8  image, and  0x0fff  for an image collected by a 12-bit camera.  Images.jl , via Colors and FixedPointNumbers, unifies these so that  1  always means saturated, no matter whether the element type is  Float64 ,  UFixed8 , or  UFixed12 .  This makes it easier to write generic algorithms and visualization code, while still allowing one to use efficient (and C-compatible) raw representations.  You can see that this image has  properties , of which there are three:  \"IMcs\" ,  \"spatialorder\"  and  \"pixelspacing\" .  We'll talk more about the latter two in the next section.  The  \"IMcs\"  is really for internal use by ImageMagick; it says that the colorspace is  \"sRGB\" , although (depending on which version of the library you have) you may see it say  \"RGB\" .  Such differences are due to  changes  in how ImageMagick handles colorspaces, and the fact that both older and newer versions of the library are still widespread.  You can retrieve the properties using  props = properties(img) . This returns the dictionary used by  img ; any modifications you make to  props  will update the properties of  img .  Likewise, given an Image  img , you can access the underlying array with  A = data(img)  This is handy for those times when you want to call an algorithm that is implemented only for  Array s. At the end, however, you may want to restore the contextual information available in an Image. While you can use the  Image  constructor directly, two alternatives can be convenient:  imgc = copyproperties(img, A)\nimgs = shareproperties(img, A)  imgc  has its own properties dictionary, initialized to be a copy of the one used by  img .  In contrast,  imgs  shares a properties dictionary with  img ; any modification to the properties of  img  will also modify them for  imgs . Use either as appropriate to your circumstance.  The Images package is designed to work with either plain arrays or with Image types\u2013-in general, though, you're probably best off leaving things as an Image, particularly if you work with movies, 3d images, or other more complex objects.", 
            "title": "Loading your first image: how images are represented"
        }, 
        {
            "location": "/#storage-order-and-changing-the-representation-of-images", 
            "text": "In the example above, the  \"spatialorder\"  property has value  [\"x\", \"y\"] . This indicates that the image data are in \"horizontal-major\" order, meaning that a pixel at spatial location  (x,y)  would be addressed as  img[x,y]  rather than  img[y,x] .  [\"y\", \"x\"]  would indicate vertical-major.  Consequently, this image is 70 pixels wide and 46 pixels high.  Images returns this image in horizontal-major order because this is how it was stored on disk.  Because the Images package is designed to scale to terabyte-sized images, a general philosophy is to work with whatever format users provide without forcing changes to the raw array representation. Consequently, when you load an image, its representation will match that used in the file.  Of course, if you prefer to work with plain arrays, you can convert it:  julia  imA = convert(Array, img);\n\njulia  summary(imA) 46x70 Array{RGB{UFixed{Uint8,8}},2}   You can see that this permuted the dimensions into vertical-major order, consistent with the column-major order with which Julia stores  Arrays . Note that this preserved the element type, returning an  Array{RGB} .  If you prefer to extract into an array of plain numbers in color-last order (typical of Matlab), you can use  julia  imsep = separate(img)\nRGB Image with:\n  data: 46x70x3 Array{UFixed{Uint8,8},3}\n  properties:\n    IMcs: sRGB\n    colorspace: RGB\n    colordim: 3\n    spatialorder:  y x\n    pixelspacing:  1 1  You can see that  \"spatialorder\"  was changed to reflect the new layout, and that two new properties were added:  \"colordim\" , which specifies which dimension of the array is used to encode color, and  \"colorspace\"  so you know how to interpret these colors.  Compare this to  julia  imr = reinterpret(UFixed8, img)\nRGB Image with:\n  data: 3x70x46 Array{UFixed{Uint8,8},3}\n  properties:\n    IMcs: sRGB\n    colorspace: RGB\n    colordim: 1\n    spatialorder:  x y\n    pixelspacing:  1 1  reinterpret  gives you a new view of the same underlying memory as  img , whereas  convert(Array, img)  and  separate(img)  create new arrays if the memory-layout needs alteration.  You can go back to using Colors to encode your image this way:  julia  imcomb = convert(Image{RGB}, imsep)\nRGB Image with:\n  data: 46x70 Array{RGB{UFixed{Uint8,8}},2}\n  properties:\n    IMcs: sRGB\n    spatialorder:  y x\n    pixelspacing:  1 1  or even change to a new colorspace like this:  julia  imhsv = convert(Image{HSV}, float32(img))\nHSV Image with:\n  data: 70x46 Array{HSV{Float32},2}\n  properties:\n    IMcs: sRGB\n    spatialorder:  x y\n    pixelspacing:  1 1  Many of the colorspaces supported by Colors (or rather its base package, ColorTypes) need a wider range of values than  [0,1] , so it's necessary to convert to floating point.  If you say  view(imhsv) , you may be surprised to see something that looks like the original RGB image. Since the colorspace is known, it converts to RGB before rendering it. If, for example, you wanted to see what a \"pure-V\" image looks like, you can do this:  imv = shareproperties(imhsv, [HSV(0, 0, imhsv[i,j].v) for i = 1:size(imhsv,1),j = 1:size(imhsv,2)])\nview(imv)  and a pure-H image like this:  imh = shareproperties(imhsv, [HSV(imhsv[i,j].h, 0.5, 0.5) for i = 1:size(imhsv,1),j = 1:size(imhsv,2)])\nview(imh)  (Hue without saturation or value generates gray or black, so we used a constant different from zero for these parameters.)     Of course, you can combine these commands, for example  A = reinterpret(Uint8, data(img))  will, for a  RGB{UFixed8}  image, return a raw 3d array.  This can be useful if you want to interact with external code (a C-library, for example).  Assuming you don't want to lose orientation information, you can wrap a returned array  B  as  shareproperties(img, B) .", 
            "title": "Storage order and changing the representation of images"
        }, 
        {
            "location": "/#other-properties-and-usage-of-units", 
            "text": "The  \"pixelspacing\"  property informs ImageView that this image has an aspect ratio 1.  In scientific or medical imaging, you can use actual units to encode this property, for example through the  SIUnits  package.  For example, if you're doing microscopy you might specify  using SIUnits\nimg[ pixelspacing ] = [0.32Micro*Meter,0.32Micro*Meter]  If you're performing three-dimensional imaging, you might set different values for the different axes:  using SIUnits.ShortUnits\nmriscan[ pixelspacing ] = [0.2mm, 0.2mm, 2mm]  ImageView includes facilities for scale bars, and by supplying your pixel spacing you can ensure that the scale bars are accurate.", 
            "title": "Other properties, and usage of Units"
        }, 
        {
            "location": "/#a-brief-demonstration-of-image-processing", 
            "text": "Now let's work through a more sophisticated example:  using Images, TestImages, ImageView\nimg = testimage( mandrill )\nview(img)\n# Let's do some blurring\nkern = ones(Float32,7,7)/49\nimgf = imfilter(img, kern)\nview(imgf)\n# Let's make an oversaturated image\nimgs = 2imgf\nview(imgs)", 
            "title": "A brief demonstration of image processing"
        }, 
        {
            "location": "/#further-documentation", 
            "text": "Detailed documentation about the design of the library and the available functions can be found in the navigation list to the right. Here are some of the topics available:   The  core  representation of images  Function reference  Overlays , a type for combining multiple grayscale arrays   into a single color array", 
            "title": "Further documentation"
        }, 
        {
            "location": "/#credits", 
            "text": "Elements of this package descend from \"image.jl\" that once lived in Julia's  extras/  directory.  That file had several authors, of which the primary were Jeff Bezanson, Stefan Kroboth, Tim Holy, Mike Nolta, and Stefan Karpinski.  This repository has been quite heavily reworked; the current package maintainer is Tim Holy, and important contributions have been made by Ron Rock, Kevin Squire, Lucas Beyer, Elliot Saba, Isaiah Norton, Daniel Perry, Waldir Pimenta, Tobias Knopp, Jason Merrill, Dahua Lin, and several others.", 
            "title": "Credits"
        }, 
        {
            "location": "/aims/", 
            "text": "Aims\n\n\nImages are very diverse.  You might be working with a single photograph, or you might be processing MRI scans from databases of hundreds of subjects.  In the former case, you might not need much information about the image; perhaps just the pixel data itself suffices.  In the latter case, you probably need to know a lot of extra details, like the patient's ID number and characteristics of the image like the physical size of a voxel in all three dimensions.\n\n\nEven the raw pixel data can come in several different flavors. For example, you might represent each pixel as a \nUint32\n because you are encoding red, green, and blue in separate 8-bit words within each integer. Visualization libraries like Cairo use these kinds of representations, and you might want to interact with those libraries efficiently.  Alternatively, perhaps you're an astronomer and your camera has such high precision that 16 bits aren't enough to encode grayscale intensities. If you're working with videos (images collected over time), you might have arrays that are too big to load into memory at once.  You still need to be able to \"talk about\" the array as a whole, but it may not be trivial to adjust the byte-level representation to match some pre-conceived storage order.\n\n\nTo handle this diversity, we've endeavored to take a \"big tent\" philosophy.  We avoid imposing a strict programming model, because we don't want to make life difficult for people who have relatively simple needs.  If you do all your image processing with plain arrays (as is typical in Matlab, for example), that should work just fine. You just have to respect certain conventions, like a \nm\n-by-\nn\n-by-\n3\n array always means an RGB image with the third dimension encoding color.  You can call the routines that are in this package, and write your own custom algorithms that assume the same format.\n\n\nBut if your images don't fit neatly into these assumptions, you can choose to represent your images using other schemes; you can then tag them with enough metadata that there's no ambiguity about the meaning of anything.  The algorithms in this package are already set to look for certain types of metadata, and adjust their behavior accordingly.\n\n\nOne of the potential downsides of flexibility is complexity; it makes it harder to write generic algorithms that work with all these different representations. We've tried to mitigate this downside by providing many short utility functions that abstract away much of the complexity.  Many algorithms require just a handful of extra lines to work generically.  Or if you just want to get something running, it usually only takes a couple of lines of code to assert that the input is in the format you expect.", 
            "title": "Aims"
        }, 
        {
            "location": "/aims/#aims", 
            "text": "Images are very diverse.  You might be working with a single photograph, or you might be processing MRI scans from databases of hundreds of subjects.  In the former case, you might not need much information about the image; perhaps just the pixel data itself suffices.  In the latter case, you probably need to know a lot of extra details, like the patient's ID number and characteristics of the image like the physical size of a voxel in all three dimensions.  Even the raw pixel data can come in several different flavors. For example, you might represent each pixel as a  Uint32  because you are encoding red, green, and blue in separate 8-bit words within each integer. Visualization libraries like Cairo use these kinds of representations, and you might want to interact with those libraries efficiently.  Alternatively, perhaps you're an astronomer and your camera has such high precision that 16 bits aren't enough to encode grayscale intensities. If you're working with videos (images collected over time), you might have arrays that are too big to load into memory at once.  You still need to be able to \"talk about\" the array as a whole, but it may not be trivial to adjust the byte-level representation to match some pre-conceived storage order.  To handle this diversity, we've endeavored to take a \"big tent\" philosophy.  We avoid imposing a strict programming model, because we don't want to make life difficult for people who have relatively simple needs.  If you do all your image processing with plain arrays (as is typical in Matlab, for example), that should work just fine. You just have to respect certain conventions, like a  m -by- n -by- 3  array always means an RGB image with the third dimension encoding color.  You can call the routines that are in this package, and write your own custom algorithms that assume the same format.  But if your images don't fit neatly into these assumptions, you can choose to represent your images using other schemes; you can then tag them with enough metadata that there's no ambiguity about the meaning of anything.  The algorithms in this package are already set to look for certain types of metadata, and adjust their behavior accordingly.  One of the potential downsides of flexibility is complexity; it makes it harder to write generic algorithms that work with all these different representations. We've tried to mitigate this downside by providing many short utility functions that abstract away much of the complexity.  Many algorithms require just a handful of extra lines to work generically.  Or if you just want to get something running, it usually only takes a couple of lines of code to assert that the input is in the format you expect.", 
            "title": "Aims"
        }, 
        {
            "location": "/core/", 
            "text": "Julia Images Guide\n\n\n\n\nThe basic types\n\n\n\n\nPlain arrays\n\n\nImages can be plain arrays, which are interpreted to be in \"Matlab format\": the first two dimensions are height (\nh\n) and width (\nw\n), a storage order here called \"vertical-major\". This ordering is inspired by the column/row index order of matrices and the desire to have a displayed image look like what one sees when a matrix is written out in text.\n\n\nIf you're working with RGB color, your best approach is to encode color as a \nColor\n, as defined in the \nColor\n package.  That package provides many utility functions for analyzing and manipulating colors.  Alternatively, you can use a third dimension of size 3, or encode your images as either \nRGB24\n or \nARGB32\n, which use an internal \nUint32\n representation of color.\n\n\nIt's worth noting that these Matlab conventions are sometimes inconvenient.  For example, the \nx\n coordinate (horizontal) is second and the \ny\n coordinate (vertical) is first; in other words, one uses \nimg[y,x]\n to address a pixel that is displayed at a particular \nx,y\n position. This often catches newcomers (and sometimes even old-timers) by surprise.  Moreover, most image file formats, cameras, and graphics libraries such as Cairo use \"horizontal-major\" storage of images, and have the color dimension first (fastest). The native Image type\u2013-which allows arbitrary ordering of the data array\u2013-permits you to use this raw representation directly, but when using plain arrays you need to permute the dimensions of the raw data array.\n\n\nThe convention that a \nm x n x 3\n array implies RGB is also problematic for anyone doing 3d imaging, and can result in hard-to-find bugs when the third dimension happens to be of size 3. For 3d imaging, the use of an Image type\u2013-perhaps converting Arrays via \ngrayim\n\u2013-is highly recommended.\n\n\nThe conventions for plain arrays are \"baked in\" via a few simple utility functions in the file \ncore.jl\n; if you really need to use plain arrays but want to work with different conventions, you can (locally) change these defaults with just a few lines. Algorithms which have been written generically should continue to work.\n\n\nHowever, a more flexible approach is to use one of the self-documenting image types.\n\n\n\n\nImage types\n\n\nAll image types should descend from \nAbstractImage\n, an abstract base type used to indicate that an array is to be interpreted as an image. If you're writing a custom image type, it is more likely that you'll want to derive from either \nAbstractImageDirect\n or \nAbstractImageIndexed\n. The former is for direct images (where intensity at a pixel is represented directly), the latter for indexed images (where intensity is looked up in a colormap table).\n\n\nIn practice, it is assumed that \nAbstractImages\n have at least two fields, called \ndata\n and \nproperties\n. (In code, you should not use these directly, instead using the functions \ndata\n and \nproperties\n to extract these.)  These are the only two fields in the first concrete image type, called \nImage\n:\n\n\ntype Image{T,N,A\n:AbstractArray} \n: AbstractImageDirect{T,N}\n    data::A\n    properties::Dict{ASCIIString,Any}\nend\n\n\n\n\ndata\n stores the actual image data, and is an \nAbstractArray\n. This fact alone is the basis for a great deal of customizability: \ndata\n might be a plain \nArray\n stored in memory, a \nSubArray\n, a memory-mapped array (which is still just an \nArray\n), a custom type that stores additional information about \"missing data\" (like bad pixels or dropped frames), or a custom type that seamlessly presents views of a large number of separate files.  One concrete example in the Images codebase is the color \nOverlay\n \ntype\n.  If you have a suitably-defined \nAbstractArray\n type, you can probably use \nImage\n without needing to create alternative \nAbstractImageDirect\n types.\n\n\nproperties\n is a dictionary, with \nString\n keys, that allows you to annotate images. More detail about this point can be found below.\n\n\nThe only other concrete image type is for indexed images:\n\n\ntype ImageCmap{T,N,A\n:AbstractArray,C\n:AbstractArray} \n: AbstractImageIndexed{T,N}\n    data::A\n    cmap::C\n    properties::Dict{ASCIIString,Any}\nend\n\n\n\n\nThe \ndata\n array here just encodes the index used to look up the color in the \ncmap\n field.\n\n\n\n\nAddressing image data\n\n\nFor any valid image type, \ndata(img)\n returns the array that corresponds to the image.  This works when \nimg\n is a plain \nArray\n (in which case no operation is performed) as well as for an \nImage\n (in which case it returns \nimg.data\n). For some image formats, Images.jl may interpret raw data with the \nFixedPointNumbers\n package. The function \nraw(img)\n can be used to recover the buffer in its raw format (e.g. \nUInt8\n). This is our first example of how to write generic algorithms.\n\n\nIf \nimg\n is an \nImage\n, then \nimg[i,j]\n looks up the value \nimg.data[i,j]\n. Assignment, \nsub\n, and \nslice\n work similarly. In other words, for indexing an \nImage\n works just as if you were using plain arrays.\n\n\nIf you load your image data using Image's \nimread\n, note that the storage order is not changed from the on-disk representation. Therefore, a 2D RGB image will most likely be stored in color-horizontal-vertical order, meaning that a pixel at \n(x,y)\n is accessed as \nimg[x,y]\n. Note that this is quite different from Matlab's default representation.\n\n\nIf you are indexing over an extended region and want to get back an \nImage\n, rather than a value or an \nArray\n, then you will want to use \ngetindexim\n, \nsubim\n, and \nsliceim\n. For the first two, the resulting image will share everything but the \ndata\n field with the original image; if you make modifications in one, the other will also be affected. For \nsliceim\n, because it can change the dimensionality some adjustments to \nproperties\n are needed; in this case a copy is made.\n\n\nOne of the properties (see below) that you can grant to images is \nspatialorder\n, which provides a name for each spatial dimension in the image. Using this feature, you can cut out regions or slices from images in the following ways:\n\n\nA = img[\nx\n, 200:400, \ny\n, 500:700]\nimgs = sliceim(img, \nz\n, 14)      # cuts out the 14th frame in a stack\n\n\n\n\nThese routines \"do the right thing\" no matter what storage order is being used.\n\n\n\n\nImage properties and accessor functions\n\n\nThe \nproperties\n dictionary can contain any information you want to store along with your images. Typically, each property is also affiliated with an accessor function of the same name.\n\n\nLet's illustrate this with one of the default properties, \n\"colorspace\"\n. The value of this property is a string, such as \n\"RGB\"\n or \n\"Gray\"\n or \n\"HSV\"\n. You can extract the value of this field using a function:\n\n\ncs = colorspace(img)\n\n\n\n\nThe reason to have a function, rather than just looking it up in the \nproperties\n dictionary, is that we can provide defaults. For example, images represented as plain \nArray\ns don't have a \nproperties\n dictionary; if we are to write generic code, we don't want to have to wonder whether this information is available. So for plain arrays, there are a number of defaults specified for the output of the \ncolorspace\n function, depending on the element type and size of the array. Likewise, images stored as \nColor\n arrays have no need of a \n\"colorspace\"\n property, because the colorspace is encoded in the type parameters.\n\n\nHere is a list of the properties supported in \ncore.jl\n:\n\n\n\n\ncolorspace\n: \"RGB\", \"RGBA\", \"Gray\", \"Binary\", \"24bit\", \"Lab\", \"HSV\", etc.  If   your image is represented as a Color array, you cannot override that   choice by specifying a \ncolorspace\n property.  (Use \nreinterpret\n if you want   to change the interpretation without changing the raw values.)\n\n\ncolordim\n: the array dimension used to store color information, or 0 if there   is no dimension corresponding to color\n\n\ntimedim\n: the array dimension used for time (i.e., sequence), or 0 for single   images\n\n\nscalei\n: a property that controls default contrast scaling upon display.   This should be a   \nMapInfo\n   value, to be used for setting the contrast upon display. In the absence of   this property, the range 0 to 1 will be used.\n\n\npixelspacing\n: the spacing between adjacent pixels along spatial dimensions\n\n\nspacedirections\n: more detailed information about the orientation of array   axes relative to an external coordinate system (see the   \nfunction reference\n).\n\n\nspatialorder\n: a string naming each spatial dimension of the array, in the   storage order of the data array.  Names can be arbitrary, but the choices \"x\"   and \"y\" have special meaning (horizontal and vertical, respectively,   irrespective of storage order).  If supplied, you must have one entry per   spatial dimension.\n\n\n\n\nIf you specify their values in the \nproperties\n dictionary, your values will be used; if not, hopefully-reasonable defaults will be chosen.\n\n\nNaturally, you can add whatever additional properties you want: you could add the date/time at which the image was captured, the patient ID, etc. The main point of having a properties dictionary, rather than a type with fixed fields, is the flexibility of adding whatever metadata you find to be useful.\n\n\n\n\nWriting generic algorithms\n\n\nLet's say you have an algorithm implemented for \nArray\ns, and you want to extend it to work on \nImage\n types. Let's consider the example of a hypothetical \nimfilter\n, written to perform kernel-based filtering in arbitrary dimensions. Let's say your \nimfilter\n looks like this:\n\n\nfunction imfilter{T,N}(A::Array{T,N}, kernel::Array{T,N}, options...)\n\n\n\n\nThe first step might be to simply provide a version for \nAbstractImage\n types:\n\n\nfunction imfilter{T,N}(img::AbstractImage{T,N}, kernel::Array{T,N}, options...)\n    out = imfilter(data(img), kernel, options...)\n    shareproperties(img, out)\nend\n\n\n\n\nNow let's say you additionally want to allow the user to filter color images\u2013-where one dimension of the array is used to encode color\u2013-with a filter of dimension \nN-1\n applied to each color channel separately. We can implement this version simultaneously for both \nImage\n types and other array types as follows:\n\n\nfunction imfilter{T,N,N1}(img::AbstractArray{T,N}, kernel::Array{T,N1}, options...)\n    cd = colordim(img)\n    if N1 != N - (cd != 0)\n        error(\nkernel has the wrong dimensionality\n)\n    end\n    out = similar(img)\n    for i = size(img, cd)\n        imsl = img[\ncolor\n, i]\n        outsl = slice(out, \ncolor\n, i)\n        copy!(outsl, imfilter(imsl, kernel, options...))\n    end\n    out\nend\n\n\n\n\nThere are other ways to achieve a similar effect; if you examine the actual implementation of \nimfilter\n, you'll see that the kernel is reshaped to be commensurate with the data array.\n\n\nThese solutions work no matter which dimension is used to store color, a feat that would be essentially impossible to achieve robustly in a generic algorithm if we didn't exploit metadata. Note also that if the user supplies an \nArray\n, s/he will get an \nArray\n back, and if using an \nImage\n will get an \nImage\n back with properties inherited from \nimg\n.\n\n\nNaturally, you can find other examples of generic implementations throughout the source code of \nImages\n.", 
            "title": "Core Concepts"
        }, 
        {
            "location": "/core/#julia-images-guide", 
            "text": "", 
            "title": "Julia Images Guide"
        }, 
        {
            "location": "/core/#the-basic-types", 
            "text": "", 
            "title": "The basic types"
        }, 
        {
            "location": "/core/#plain-arrays", 
            "text": "Images can be plain arrays, which are interpreted to be in \"Matlab format\": the first two dimensions are height ( h ) and width ( w ), a storage order here called \"vertical-major\". This ordering is inspired by the column/row index order of matrices and the desire to have a displayed image look like what one sees when a matrix is written out in text.  If you're working with RGB color, your best approach is to encode color as a  Color , as defined in the  Color  package.  That package provides many utility functions for analyzing and manipulating colors.  Alternatively, you can use a third dimension of size 3, or encode your images as either  RGB24  or  ARGB32 , which use an internal  Uint32  representation of color.  It's worth noting that these Matlab conventions are sometimes inconvenient.  For example, the  x  coordinate (horizontal) is second and the  y  coordinate (vertical) is first; in other words, one uses  img[y,x]  to address a pixel that is displayed at a particular  x,y  position. This often catches newcomers (and sometimes even old-timers) by surprise.  Moreover, most image file formats, cameras, and graphics libraries such as Cairo use \"horizontal-major\" storage of images, and have the color dimension first (fastest). The native Image type\u2013-which allows arbitrary ordering of the data array\u2013-permits you to use this raw representation directly, but when using plain arrays you need to permute the dimensions of the raw data array.  The convention that a  m x n x 3  array implies RGB is also problematic for anyone doing 3d imaging, and can result in hard-to-find bugs when the third dimension happens to be of size 3. For 3d imaging, the use of an Image type\u2013-perhaps converting Arrays via  grayim \u2013-is highly recommended.  The conventions for plain arrays are \"baked in\" via a few simple utility functions in the file  core.jl ; if you really need to use plain arrays but want to work with different conventions, you can (locally) change these defaults with just a few lines. Algorithms which have been written generically should continue to work.  However, a more flexible approach is to use one of the self-documenting image types.", 
            "title": "Plain arrays"
        }, 
        {
            "location": "/core/#image-types", 
            "text": "All image types should descend from  AbstractImage , an abstract base type used to indicate that an array is to be interpreted as an image. If you're writing a custom image type, it is more likely that you'll want to derive from either  AbstractImageDirect  or  AbstractImageIndexed . The former is for direct images (where intensity at a pixel is represented directly), the latter for indexed images (where intensity is looked up in a colormap table).  In practice, it is assumed that  AbstractImages  have at least two fields, called  data  and  properties . (In code, you should not use these directly, instead using the functions  data  and  properties  to extract these.)  These are the only two fields in the first concrete image type, called  Image :  type Image{T,N,A :AbstractArray}  : AbstractImageDirect{T,N}\n    data::A\n    properties::Dict{ASCIIString,Any}\nend  data  stores the actual image data, and is an  AbstractArray . This fact alone is the basis for a great deal of customizability:  data  might be a plain  Array  stored in memory, a  SubArray , a memory-mapped array (which is still just an  Array ), a custom type that stores additional information about \"missing data\" (like bad pixels or dropped frames), or a custom type that seamlessly presents views of a large number of separate files.  One concrete example in the Images codebase is the color  Overlay   type .  If you have a suitably-defined  AbstractArray  type, you can probably use  Image  without needing to create alternative  AbstractImageDirect  types.  properties  is a dictionary, with  String  keys, that allows you to annotate images. More detail about this point can be found below.  The only other concrete image type is for indexed images:  type ImageCmap{T,N,A :AbstractArray,C :AbstractArray}  : AbstractImageIndexed{T,N}\n    data::A\n    cmap::C\n    properties::Dict{ASCIIString,Any}\nend  The  data  array here just encodes the index used to look up the color in the  cmap  field.", 
            "title": "Image types"
        }, 
        {
            "location": "/core/#addressing-image-data", 
            "text": "For any valid image type,  data(img)  returns the array that corresponds to the image.  This works when  img  is a plain  Array  (in which case no operation is performed) as well as for an  Image  (in which case it returns  img.data ). For some image formats, Images.jl may interpret raw data with the  FixedPointNumbers  package. The function  raw(img)  can be used to recover the buffer in its raw format (e.g.  UInt8 ). This is our first example of how to write generic algorithms.  If  img  is an  Image , then  img[i,j]  looks up the value  img.data[i,j] . Assignment,  sub , and  slice  work similarly. In other words, for indexing an  Image  works just as if you were using plain arrays.  If you load your image data using Image's  imread , note that the storage order is not changed from the on-disk representation. Therefore, a 2D RGB image will most likely be stored in color-horizontal-vertical order, meaning that a pixel at  (x,y)  is accessed as  img[x,y] . Note that this is quite different from Matlab's default representation.  If you are indexing over an extended region and want to get back an  Image , rather than a value or an  Array , then you will want to use  getindexim ,  subim , and  sliceim . For the first two, the resulting image will share everything but the  data  field with the original image; if you make modifications in one, the other will also be affected. For  sliceim , because it can change the dimensionality some adjustments to  properties  are needed; in this case a copy is made.  One of the properties (see below) that you can grant to images is  spatialorder , which provides a name for each spatial dimension in the image. Using this feature, you can cut out regions or slices from images in the following ways:  A = img[ x , 200:400,  y , 500:700]\nimgs = sliceim(img,  z , 14)      # cuts out the 14th frame in a stack  These routines \"do the right thing\" no matter what storage order is being used.", 
            "title": "Addressing image data"
        }, 
        {
            "location": "/core/#image-properties-and-accessor-functions", 
            "text": "The  properties  dictionary can contain any information you want to store along with your images. Typically, each property is also affiliated with an accessor function of the same name.  Let's illustrate this with one of the default properties,  \"colorspace\" . The value of this property is a string, such as  \"RGB\"  or  \"Gray\"  or  \"HSV\" . You can extract the value of this field using a function:  cs = colorspace(img)  The reason to have a function, rather than just looking it up in the  properties  dictionary, is that we can provide defaults. For example, images represented as plain  Array s don't have a  properties  dictionary; if we are to write generic code, we don't want to have to wonder whether this information is available. So for plain arrays, there are a number of defaults specified for the output of the  colorspace  function, depending on the element type and size of the array. Likewise, images stored as  Color  arrays have no need of a  \"colorspace\"  property, because the colorspace is encoded in the type parameters.  Here is a list of the properties supported in  core.jl :   colorspace : \"RGB\", \"RGBA\", \"Gray\", \"Binary\", \"24bit\", \"Lab\", \"HSV\", etc.  If   your image is represented as a Color array, you cannot override that   choice by specifying a  colorspace  property.  (Use  reinterpret  if you want   to change the interpretation without changing the raw values.)  colordim : the array dimension used to store color information, or 0 if there   is no dimension corresponding to color  timedim : the array dimension used for time (i.e., sequence), or 0 for single   images  scalei : a property that controls default contrast scaling upon display.   This should be a    MapInfo    value, to be used for setting the contrast upon display. In the absence of   this property, the range 0 to 1 will be used.  pixelspacing : the spacing between adjacent pixels along spatial dimensions  spacedirections : more detailed information about the orientation of array   axes relative to an external coordinate system (see the    function reference ).  spatialorder : a string naming each spatial dimension of the array, in the   storage order of the data array.  Names can be arbitrary, but the choices \"x\"   and \"y\" have special meaning (horizontal and vertical, respectively,   irrespective of storage order).  If supplied, you must have one entry per   spatial dimension.   If you specify their values in the  properties  dictionary, your values will be used; if not, hopefully-reasonable defaults will be chosen.  Naturally, you can add whatever additional properties you want: you could add the date/time at which the image was captured, the patient ID, etc. The main point of having a properties dictionary, rather than a type with fixed fields, is the flexibility of adding whatever metadata you find to be useful.", 
            "title": "Image properties and accessor functions"
        }, 
        {
            "location": "/core/#writing-generic-algorithms", 
            "text": "Let's say you have an algorithm implemented for  Array s, and you want to extend it to work on  Image  types. Let's consider the example of a hypothetical  imfilter , written to perform kernel-based filtering in arbitrary dimensions. Let's say your  imfilter  looks like this:  function imfilter{T,N}(A::Array{T,N}, kernel::Array{T,N}, options...)  The first step might be to simply provide a version for  AbstractImage  types:  function imfilter{T,N}(img::AbstractImage{T,N}, kernel::Array{T,N}, options...)\n    out = imfilter(data(img), kernel, options...)\n    shareproperties(img, out)\nend  Now let's say you additionally want to allow the user to filter color images\u2013-where one dimension of the array is used to encode color\u2013-with a filter of dimension  N-1  applied to each color channel separately. We can implement this version simultaneously for both  Image  types and other array types as follows:  function imfilter{T,N,N1}(img::AbstractArray{T,N}, kernel::Array{T,N1}, options...)\n    cd = colordim(img)\n    if N1 != N - (cd != 0)\n        error( kernel has the wrong dimensionality )\n    end\n    out = similar(img)\n    for i = size(img, cd)\n        imsl = img[ color , i]\n        outsl = slice(out,  color , i)\n        copy!(outsl, imfilter(imsl, kernel, options...))\n    end\n    out\nend  There are other ways to achieve a similar effect; if you examine the actual implementation of  imfilter , you'll see that the kernel is reshaped to be commensurate with the data array.  These solutions work no matter which dimension is used to store color, a feat that would be essentially impossible to achieve robustly in a generic algorithm if we didn't exploit metadata. Note also that if the user supplies an  Array , s/he will get an  Array  back, and if using an  Image  will get an  Image  back with properties inherited from  img .  Naturally, you can find other examples of generic implementations throughout the source code of  Images .", 
            "title": "Writing generic algorithms"
        }, 
        {
            "location": "/overlays/", 
            "text": "Overlays\n\n\nFrequently one wants to combine two (or more) grayscale images into a single colorized image.  \nImages\n defines an \nAbstractArray\n type, \nOverlay\n, making this straightforward.\n\n\nTo create an overlay, use the following syntax:\n\n\nO = Overlay((gray1,gray2,...), (color1,color2,...), (clim1,clim2,...))\n\n\n\n\nHere \ngray1\n and \ngray2\n are the arrays representing individual \"channels\" of information, each equivalent to a grayscale image.  \ncolor1\n and \ncolor2\n are the \nColors\n that will be used for the corresponding grayscale arrays; for example, to put \ngray1\n in the red channel, you'd use \ncolor1 = RGB(1,0,0)\n.  (You can choose any RGB value you want, it doesn't have to be a \"pure\" RGB channel.)  Finally, \nclim1\n and \nclim2\n represent the intensity-scaling applied to each image; setting \nclim1 = (400,3000)\n would send any values in \ngray1\n less than 400 to black, and any values bigger than 3000 to red, with other values between encoded linearly.\n\n\nOnce constructed, an \nOverlay\n acts as an \nArray{RGB}\n. You can embed it in an \nImage\n or just use it directly.", 
            "title": "Overlays"
        }, 
        {
            "location": "/overlays/#overlays", 
            "text": "Frequently one wants to combine two (or more) grayscale images into a single colorized image.   Images  defines an  AbstractArray  type,  Overlay , making this straightforward.  To create an overlay, use the following syntax:  O = Overlay((gray1,gray2,...), (color1,color2,...), (clim1,clim2,...))  Here  gray1  and  gray2  are the arrays representing individual \"channels\" of information, each equivalent to a grayscale image.   color1  and  color2  are the  Colors  that will be used for the corresponding grayscale arrays; for example, to put  gray1  in the red channel, you'd use  color1 = RGB(1,0,0) .  (You can choose any RGB value you want, it doesn't have to be a \"pure\" RGB channel.)  Finally,  clim1  and  clim2  represent the intensity-scaling applied to each image; setting  clim1 = (400,3000)  would send any values in  gray1  less than 400 to black, and any values bigger than 3000 to red, with other values between encoded linearly.  Once constructed, an  Overlay  acts as an  Array{RGB} . You can embed it in an  Image  or just use it directly.", 
            "title": "Overlays"
        }, 
        {
            "location": "/function_reference/", 
            "text": "Function Reference\n\n\nBelow, \n[]\n in an argument list means an optional argument.\n\n\n\n\nImage construction\n\n\n@{Image} @{ImageCmap}\n\n\nconvert(Image, A)\nconvert(Array, img)\nconvert(Image{HSV}, img)\n\n\n\n\nThe first creates a 2d image from an array, setting up default properties. The data array is assumed to be in \"vertical-major\" order, and an m-by-n-by-3 array will be assumed to encode color along its third dimension.\n\n\nconvert(Array, img)\n works in the opposite direction, permuting dimensions (if needed) to put it in Matlab-standard storage order.\n\n\nThe third syntax allows you to convert from one colorspace to another.\n\n\n@{     grayim     colorim     copyproperties     shareproperties     similar     Overlay     OverlayImage }\n\n\n\n\nAccessing image data\n\n\n@{     data     raw     separate }\n\n\nimg\n\n\nimg[i, j, k,...]\nimg[\nx\n, 100:200, \ny\n, 400:600]\n\n\n\n\nreturn image data as an array. The latter syntax allows you to address dimensions by name, irrespective of the storage order. The returned values have the same storage order as the parent.\n\n\n@{     getindexim }\n\n\nsub and slice\n\n\nsub(img, i, j, k, ...)\nsub(img, \nx\n, 100:200, \ny\n, 400:600)\nslice(img, i, j, k, ...)\nslice(img, \nx\n, 15, \ny\n, 400:600)\n\n\n\n\nreturns a \nSubArray\n of image data, with the ordinary meanings of \nsub\n and \nslice\n.\n\n\nsubim and sliceim\n\n\nsubim(img, i, j, k, ...)\nsubim(img, \nx\n, 100:200, \ny\n, 400:600)\nsliceim(img, i, j, k, ...)\nsubim(img, \nx\n, 15, \ny\n, 400:600)\n\n\n\n\nreturns an \nImage\n with \nSubArray\n data.\n\n\n\n\nProperties dictionary-like interface\n\n\nUnless specified, these functions work on both plain arrays (when properties can be inferred), and on \nImage\n types.\n\n\nval = img[\npropertyname\n]\nimg[\npropertyname\n] = val\n\n\n\n\nget and set, respectively, the value of a property. These work only for \nImage\n types.\n\n\nhaskey\n\n\nhaskey(img, \npropertyname\n)\n\n\n\n\nTests whether the named property exists. Returns false for \nArray\ns.\n\n\nget\n\n\nget(img, \npropertyname\n, defaultvalue)\n\n\n\n\nGets the named property, or returns the default if not present. For \nArray\n, the default is always returned.\n\n\n\n\nProperties accessor-function interface\n\n\nUnless specified, these functions work on both plain arrays (when properties can be inferred), and on \nImage\n types.\n\n\n@{     assert2d     assert_scalar_color     assert_timedim_last     assert_xfirst     colordim     colorspace     coords_spatial     height     isdirect     isxfirst     isyfirst     pixelspacing     spacedirections     nimages     sdims }\n\n\nsize\n\n\nsize(img, 2)\nsize(img, \nt\n)\n\n\n\n\nObtains the size of the specified dimension, even for dimensions specified by name. See also \nnimages\n, \nsize_spatial\n, \nwidth\n, \nheight\n, and \nwidthheight\n.\n\n\n@{     size_spatial     spatialorder     spatialpermutation     spatialproperties     storageorder     timedim     width     widthheight }\n\n\n\n\nElement transformation and intensity scaling\n\n\nMany images require some type of transformation before you can use or view them. For example, visualization libraries work in terms of 8-bit data, so if you're using a 16-bit scientific camera, your image values will need to be scaled before display.\n\n\nOne can directly rescale the pixel intensities in the image array.  In general, element-wise transformations are handled by \nmap\n or \nmap!\n, where the latter is used when you want to provide a pre-allocated output.  You can use an anonymous function of your own design, or, if speed is paramount, the \"anonymous functions\" of the \nFastAnonymous\n package.\n\n\nImages also supports \"lazy transformations.\" When loading a very large image, (e.g., loaded by memory-mapping) you may use or view just a small portion of it. In such cases, it would be quite wasteful to force transformation of the entire image, and indeed on might exhaust available memory or need to write a new file on disk.  \nImages\n supports lazy-evaluation scaling through the \nMapInfo\n abstract type.  The basic syntax is\n\n\nvalout = map(mapi::MapInfo, valin)\n\n\n\n\nHere \nval\n can refer to a single pixel's data, or to the entire image array. The \nmapi\n input is a type that determines how the input value is scale and converted to a new type.\n\n\n\n\nMapInfo\n\n\nHere is how to directly construct the major concrete \nMapInfo\n types:\n\n\n\n\n\n\nMapNone(T)\n, indicating that the only form of scaling is conversion   to type T.  This can throw an error if a value \nx\n cannot be   represented as an object of type \nT\n, e.g., \nmap(MapNone{U8}, 1.2)\n.\n\n\n\n\n\n\nClampMin(T, minvalue)\n, \nClampMax(T, maxvalue)\n, and   \nClampMinMax(T, minvalue, maxvalue)\n create \nMapInfo\n objects that   clamp pixel values at the specified min, max, and min/max values,   respectively, before converting to type \nT\n. Clamping is equivalent   to \nclampedval = min(max(val, minvalue), maxvalue)\n.\n\n\n\n\n\n\nBitShift(T, N)\n or \nBitShift{T,N}()\n, for scaling by bit-shift operators.   \nN\n specifies the number of bits to right-shift by.  For example you could   convert a 14-bit image to 8-bits using \nBitShift(Uint8, 6)\n.  In general this   will be faster than using multiplication.\n\n\n\n\n\n\nScaleMinMax(T, min, max, [scalefactor])\n clamps the image at the specified   min/max values, subtracts the min value, scales the result by multiplying by   \nscalefactor\n, and finally converts the type.  If \nscalefactor\n is not   specified, it defaults to scaling the range \n[min,max]\n to \n[0,1]\n.\n\n\n\n\n\n\nScaleAutoMinMax(T)\n will cause images to be dynamically scaled to their   specific min/max values, using the same algorithm for \nScaleMinMax\n. When   displaying a movie, the min/max will be recalculated for each frame, so this   can result in inconsistent contrast scaling.\n\n\n\n\n\n\nScaleSigned(T, scalefactor)\n multiplies the image by the scalefactor, then   clamps to the range \n[-1,1]\n. If \nT\n is a floating-point type, it stays in   this representation.  If \nT\n is \nRGB24\n or \nRGB{UFixed8}\n, then it is encoded   as a magenta (positive)/green (negative) image.\n\n\n\n\n\n\nThere are also convenience functions:\n\n\n@{     imstretch     sc     MapInfo     mapinfo }\n\n\n\n\nColor conversion\n\n\nconvert\n\n\nconvert(Image{Color}, img)\n\n\n\n\nas described above. Use \nconvert(Image{Gray}, img)\n to calculate a grayscale representation of a color image using the \nRec 601 luma\n.\n\n\nmap\n\n\nmap(mapi, img)\nmap!(mapi, dest, img)\n\n\n\n\ncan be used to specify both the form of the result and the algorithm used.\n\n\n\n\nImage I/O\n\n\nImage loading and saving is handled by the \nFileIO\n package.\n\n\n\n\nImage algorithms\n\n\nYou can perform arithmetic with \nImage\ns and \nColor\ns. Algorithms also include the following functions:\n\n\n\n\nLinear filtering and padding\n\n\n@{     imfilter     imfilter!     imfilter_fft     imfilter_gaussian     imfilter_LoG     imgradients     magnitude     phase     orientation     magnitude_phase     imedge     thin_edges     forwarddiffx     forwarddiffy     backdiffx     backdiffy     padarray }\n\n\n\n\nFeature Extraction\n\n\n@{     blob_LoG     findlocalmaxima     findlocalminima }\n\n\n\n\nFiltering kernels\n\n\n@{     gaussian2d     imaverage     imdog     imlaplacian     imlog     sobel     prewitt     ando3     ando4     ando5 }\n\n\n\n\nNonlinear filtering and transformation\n\n\n@{     imROF     imcorner }\n\n\n\n\nResizing\n\n\n@{     restrict }\n\n\n\n\nImage statistics\n\n\n@{     minfinite     maxfinite     maxabsfinite     meanfinite     ssd     ssdn     sad     sadn }\n\n\n\n\nMorphological operations\n\n\n@{     dilate     erode     opening     closing     tophat     bothat     morphogradient     morpholaplace     label_components     component_boxes     component_lengths     component_indices     component_subscripts     component_centroids }\n\n\n\n\nPhantoms\n\n\n@{     shepp_logan }", 
            "title": "Function Reference"
        }, 
        {
            "location": "/function_reference/#function-reference", 
            "text": "Below,  []  in an argument list means an optional argument.", 
            "title": "Function Reference"
        }, 
        {
            "location": "/function_reference/#image-construction", 
            "text": "@{Image} @{ImageCmap}  convert(Image, A)\nconvert(Array, img)\nconvert(Image{HSV}, img)  The first creates a 2d image from an array, setting up default properties. The data array is assumed to be in \"vertical-major\" order, and an m-by-n-by-3 array will be assumed to encode color along its third dimension.  convert(Array, img)  works in the opposite direction, permuting dimensions (if needed) to put it in Matlab-standard storage order.  The third syntax allows you to convert from one colorspace to another.  @{     grayim     colorim     copyproperties     shareproperties     similar     Overlay     OverlayImage }", 
            "title": "Image construction"
        }, 
        {
            "location": "/function_reference/#accessing-image-data", 
            "text": "@{     data     raw     separate }  img  img[i, j, k,...]\nimg[ x , 100:200,  y , 400:600]  return image data as an array. The latter syntax allows you to address dimensions by name, irrespective of the storage order. The returned values have the same storage order as the parent.  @{     getindexim }  sub and slice  sub(img, i, j, k, ...)\nsub(img,  x , 100:200,  y , 400:600)\nslice(img, i, j, k, ...)\nslice(img,  x , 15,  y , 400:600)  returns a  SubArray  of image data, with the ordinary meanings of  sub  and  slice .  subim and sliceim  subim(img, i, j, k, ...)\nsubim(img,  x , 100:200,  y , 400:600)\nsliceim(img, i, j, k, ...)\nsubim(img,  x , 15,  y , 400:600)  returns an  Image  with  SubArray  data.", 
            "title": "Accessing image data"
        }, 
        {
            "location": "/function_reference/#properties-dictionary-like-interface", 
            "text": "Unless specified, these functions work on both plain arrays (when properties can be inferred), and on  Image  types.  val = img[ propertyname ]\nimg[ propertyname ] = val  get and set, respectively, the value of a property. These work only for  Image  types.  haskey  haskey(img,  propertyname )  Tests whether the named property exists. Returns false for  Array s.  get  get(img,  propertyname , defaultvalue)  Gets the named property, or returns the default if not present. For  Array , the default is always returned.", 
            "title": "Properties dictionary-like interface"
        }, 
        {
            "location": "/function_reference/#properties-accessor-function-interface", 
            "text": "Unless specified, these functions work on both plain arrays (when properties can be inferred), and on  Image  types.  @{     assert2d     assert_scalar_color     assert_timedim_last     assert_xfirst     colordim     colorspace     coords_spatial     height     isdirect     isxfirst     isyfirst     pixelspacing     spacedirections     nimages     sdims }  size  size(img, 2)\nsize(img,  t )  Obtains the size of the specified dimension, even for dimensions specified by name. See also  nimages ,  size_spatial ,  width ,  height , and  widthheight .  @{     size_spatial     spatialorder     spatialpermutation     spatialproperties     storageorder     timedim     width     widthheight }", 
            "title": "Properties accessor-function interface"
        }, 
        {
            "location": "/function_reference/#element-transformation-and-intensity-scaling", 
            "text": "Many images require some type of transformation before you can use or view them. For example, visualization libraries work in terms of 8-bit data, so if you're using a 16-bit scientific camera, your image values will need to be scaled before display.  One can directly rescale the pixel intensities in the image array.  In general, element-wise transformations are handled by  map  or  map! , where the latter is used when you want to provide a pre-allocated output.  You can use an anonymous function of your own design, or, if speed is paramount, the \"anonymous functions\" of the  FastAnonymous  package.  Images also supports \"lazy transformations.\" When loading a very large image, (e.g., loaded by memory-mapping) you may use or view just a small portion of it. In such cases, it would be quite wasteful to force transformation of the entire image, and indeed on might exhaust available memory or need to write a new file on disk.   Images  supports lazy-evaluation scaling through the  MapInfo  abstract type.  The basic syntax is  valout = map(mapi::MapInfo, valin)  Here  val  can refer to a single pixel's data, or to the entire image array. The  mapi  input is a type that determines how the input value is scale and converted to a new type.", 
            "title": "Element transformation and intensity scaling"
        }, 
        {
            "location": "/function_reference/#mapinfo", 
            "text": "Here is how to directly construct the major concrete  MapInfo  types:    MapNone(T) , indicating that the only form of scaling is conversion   to type T.  This can throw an error if a value  x  cannot be   represented as an object of type  T , e.g.,  map(MapNone{U8}, 1.2) .    ClampMin(T, minvalue) ,  ClampMax(T, maxvalue) , and    ClampMinMax(T, minvalue, maxvalue)  create  MapInfo  objects that   clamp pixel values at the specified min, max, and min/max values,   respectively, before converting to type  T . Clamping is equivalent   to  clampedval = min(max(val, minvalue), maxvalue) .    BitShift(T, N)  or  BitShift{T,N}() , for scaling by bit-shift operators.    N  specifies the number of bits to right-shift by.  For example you could   convert a 14-bit image to 8-bits using  BitShift(Uint8, 6) .  In general this   will be faster than using multiplication.    ScaleMinMax(T, min, max, [scalefactor])  clamps the image at the specified   min/max values, subtracts the min value, scales the result by multiplying by    scalefactor , and finally converts the type.  If  scalefactor  is not   specified, it defaults to scaling the range  [min,max]  to  [0,1] .    ScaleAutoMinMax(T)  will cause images to be dynamically scaled to their   specific min/max values, using the same algorithm for  ScaleMinMax . When   displaying a movie, the min/max will be recalculated for each frame, so this   can result in inconsistent contrast scaling.    ScaleSigned(T, scalefactor)  multiplies the image by the scalefactor, then   clamps to the range  [-1,1] . If  T  is a floating-point type, it stays in   this representation.  If  T  is  RGB24  or  RGB{UFixed8} , then it is encoded   as a magenta (positive)/green (negative) image.    There are also convenience functions:  @{     imstretch     sc     MapInfo     mapinfo }", 
            "title": "MapInfo"
        }, 
        {
            "location": "/function_reference/#color-conversion", 
            "text": "convert  convert(Image{Color}, img)  as described above. Use  convert(Image{Gray}, img)  to calculate a grayscale representation of a color image using the  Rec 601 luma .  map  map(mapi, img)\nmap!(mapi, dest, img)  can be used to specify both the form of the result and the algorithm used.", 
            "title": "Color conversion"
        }, 
        {
            "location": "/function_reference/#image-io", 
            "text": "Image loading and saving is handled by the  FileIO  package.", 
            "title": "Image I/O"
        }, 
        {
            "location": "/function_reference/#image-algorithms", 
            "text": "You can perform arithmetic with  Image s and  Color s. Algorithms also include the following functions:", 
            "title": "Image algorithms"
        }, 
        {
            "location": "/function_reference/#linear-filtering-and-padding", 
            "text": "@{     imfilter     imfilter!     imfilter_fft     imfilter_gaussian     imfilter_LoG     imgradients     magnitude     phase     orientation     magnitude_phase     imedge     thin_edges     forwarddiffx     forwarddiffy     backdiffx     backdiffy     padarray }", 
            "title": "Linear filtering and padding"
        }, 
        {
            "location": "/function_reference/#feature-extraction", 
            "text": "@{     blob_LoG     findlocalmaxima     findlocalminima }", 
            "title": "Feature Extraction"
        }, 
        {
            "location": "/function_reference/#filtering-kernels", 
            "text": "@{     gaussian2d     imaverage     imdog     imlaplacian     imlog     sobel     prewitt     ando3     ando4     ando5 }", 
            "title": "Filtering kernels"
        }, 
        {
            "location": "/function_reference/#nonlinear-filtering-and-transformation", 
            "text": "@{     imROF     imcorner }", 
            "title": "Nonlinear filtering and transformation"
        }, 
        {
            "location": "/function_reference/#resizing", 
            "text": "@{     restrict }", 
            "title": "Resizing"
        }, 
        {
            "location": "/function_reference/#image-statistics", 
            "text": "@{     minfinite     maxfinite     maxabsfinite     meanfinite     ssd     ssdn     sad     sadn }", 
            "title": "Image statistics"
        }, 
        {
            "location": "/function_reference/#morphological-operations", 
            "text": "@{     dilate     erode     opening     closing     tophat     bothat     morphogradient     morpholaplace     label_components     component_boxes     component_lengths     component_indices     component_subscripts     component_centroids }", 
            "title": "Morphological operations"
        }, 
        {
            "location": "/function_reference/#phantoms", 
            "text": "@{     shepp_logan }", 
            "title": "Phantoms"
        }
    ]
}